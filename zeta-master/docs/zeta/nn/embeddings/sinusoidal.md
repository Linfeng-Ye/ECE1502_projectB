# `SinusoidalEmbeddings` Documentation

## Table of Contents
1. [Introduction](#introduction)
2. [Purpose and Functionality](#purpose-and-functionality)
3. [Class: `SinusoidalEmbeddings`](#class-sinusoidalembeddings)
   - [Initialization](#initialization)
   - [Parameters](#parameters)
   - [Forward Method](#forward-method)
4. [Function: `rotate_half`](#function-rotate_half)
5. [Function: `apply_rotary_pos_emb`](#function-apply_rotary_pos_emb)
6. [Usage Examples](#usage-examples)
   - [Using the `SinusoidalEmbeddings` Class](#using-the-sinusoidalembeddings-class)
   - [Using the `rotate_half` Function](#using-the-rotate_half-function)
   - [Using the `apply_rotary_pos_emb` Function](#using-the-apply_rotary_pos_emb-function)
7. [Additional Information](#additional-information)
   - [Sinusoidal Positional Embeddings](#sinusoidal-positional-embeddings)
   - [Rotary Positional Embeddings](#rotary-positional-embeddings)
8. [References](#references)

---

## 1. Introduction <a name="introduction"></a>

Welcome to the Zeta documentation! This documentation provides an in-depth explanation of the `SinusoidalEmbeddings` class and related functions. Zeta is a PyTorch library that aims to simplify complex deep learning tasks. In this documentation, we will explore how `SinusoidalEmbeddings` and associated functions work and how they can be applied to various scenarios.

---

## 2. Purpose and Functionality <a name="purpose-and-functionality"></a>

The `SinusoidalEmbeddings` class is designed to generate sinusoidal positional embeddings for sequences in transformer-based models. These embeddings are essential for self-attention mechanisms to understand the positions of elements within a sequence. Additionally, this documentation covers the `rotate_half` and `apply_rotary_pos_emb` functions, which help apply positional embeddings to input data.

---

## 3. Class: `SinusoidalEmbeddings` <a name="class-sinusoidalembeddings"></a>

The `SinusoidalEmbeddings` class generates sinusoidal positional embeddings. It provides flexibility in configuring the embeddings and can be used to generate both basic sinusoidal embeddings and rotary positional embeddings.

### Initialization <a name="initialization"></a>

To create an instance of the `SinusoidalEmbeddings` class, you need to specify the following parameters:

```python
SinusoidalEmbeddings(dim, scale_base=None, use_xpos=False)
```

### Parameters <a name="parameters"></a>

- `dim` (int): The dimensionality of the embeddings.

- `scale_base` (float or None, optional): The scale base for rotary positional embeddings. Must be defined if `use_xpos` is set to `True`. Default is `None`.

- `use_xpos` (bool, optional): Whether to use positional information. If `True`, positional embeddings will be generated. Default is `False`.

### Forward Method <a name="forward-method"></a>

The `forward` method of the `SinusoidalEmbeddings` class generates sinusoidal positional embeddings based on the input sequence length. It returns two tensors: frequency embeddings and scale embeddings.

```python
def forward(x):
    # ...
    return freqs, scale
```

---

## 4. Function: `rotate_half` <a name="function-rotate_half"></a>

The `rotate_half` function is used to rotate input data by 180 degrees along the last dimension. It is a useful operation when working with rotary positional embeddings.

### Parameters <a name="parameters"></a>

- `x` (Tensor): The input tensor to be rotated.

### Usage Example <a name="using-the-rotate_half-function"></a>

```python
import torch

from zeta import rotate_half

# Create an input tensor
x = torch.randn(2, 3, 4)

# Rotate the input tensor
rotated_x = rotate_half(x)
```

---

## 5. Function: `apply_rotary_pos_emb` <a name="function-apply_rotary_pos_emb"></a>

The `apply_rotary_pos_emb` function applies rotary positional embeddings to input query and key tensors. It takes care of the angular transformations needed for rotary embeddings.

### Parameters <a name="parameters"></a>

- `q` (Tensor): The query tensor to which rotary positional embeddings will be applied.

- `k` (Tensor): The key tensor to which rotary positional embeddings will be applied.

- `freqs` (Tensor): The frequency embeddings generated by the `SinusoidalEmbeddings` class.

- `scale` (Tensor or float): The scale embeddings for rotary positional embeddings.

### Usage Example <a name="using-the-apply_rotary_pos_emb-function"></a>

```python
import torch

from zeta import apply_rotary_pos_emb

# Create query and key tensors
q = torch.randn(2, 3, 4)
k = torch.randn(2, 3, 4)

# Generate frequency and scale embeddings using SinusoidalEmbeddings

# Apply rotary positional embeddings
q_emb, k_emb = apply_rotary_pos_emb(q, k, freqs, scale)
```

---

## 6. Usage Examples <a name="usage-examples"></a>

Let's explore some usage examples of the `SinusoidalEmbeddings` class and associated functions to understand how they can be used effectively.

### Using the `SinusoidalEmbeddings` Class <a name="using-the-sinusoidalembeddings-class"></a>

```python
import torch

from zeta import SinusoidalEmbeddings

# Create an instance of SinusoidalEmbeddings
positional_embedding = SinusoidalEmbeddings(dim=512, use_xpos=True, scale_base=1000)

# Create an input sequence tensor
sequence = torch.randn(1, 10, 512)

# Generate positional embeddings
freqs, scale = positional_embedding(sequence)
```

### Using the `rotate_half` Function <a name="using-the-rotate_half-function"></a>

This example demonstrates how to use the `rotate_half` function:

```python
import torch

from zeta.nn import rotate_half

# Create an input tensor
x = torch.randn(2, 3, 4)

# Rotate the input tensor
rotated_x = rotate_half(x)
```

### Using the `apply_rotary_pos_emb` Function <a name="using-the-apply_rotary_pos_emb-function"></a>

This example demonstrates how to apply rotary positional embeddings using the `apply_rotary_pos_emb` function:

```python
import torch

from zeta.nn import rotate_half

# Create query and key tensors
q = torch.randn(2, 3, 4)
k = torch.randn(2, 3, 4)

# Generate frequency and scale embeddings using SinusoidalEmbeddings

# Apply rotary positional embeddings
q_emb, k_emb = apply_rotary_pos_emb(q, k, freqs, scale)
```

---

## 7. Additional Information <a name="additional-information"></a>

### Sinusoidal Positional Embeddings <a name="sinusoidal

-positional-embeddings"></a>

Sinusoidal positional embeddings are essential for transformer-based models to understand the positions of elements within a sequence. They provide information about the order and location of tokens in the input sequence.

### Rotary Positional Embeddings <a name="rotary-positional-embeddings"></a>

Rotary positional embeddings are a specialized type of positional embedding that are particularly useful in some transformer variants. They involve angular transformations to capture positional information in a unique way.

---

## 8. References <a name="references"></a>

For further information on positional embeddings in transformers and related concepts, you can refer to the following resources:

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - The original transformer paper introducing positional embeddings.

- [Rotary Position Embeddings](https://arxiv.org/abs/2104.09864) - A research paper discussing rotary positional embeddings and their advantages.

- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html) - Official PyTorch documentation for related functions and modules.

This documentation provides a comprehensive overview of the Zeta library's `SinusoidalEmbeddings` class and associated functions. It aims to help you understand the purpose, functionality, and usage of these components for positional embeddings in transformer-based models.