{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install fvcore\n",
        "!pip install torchist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ph8CrZJSGqC",
        "outputId": "319a7102-2d50-4a8c-9200-0d0775c297fe"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fvcore in /usr/local/lib/python3.10/dist-packages (0.1.5.post20221221)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore) (1.26.4)\n",
            "Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.1.8)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore) (4.66.6)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (2.5.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore) (11.0.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.9.0)\n",
            "Requirement already satisfied: iopath>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.1.10)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from iopath>=0.1.7->fvcore) (4.12.2)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath>=0.1.7->fvcore) (3.0.0)\n",
            "Requirement already satisfied: torchist in /usr/local/lib/python3.10/dist-packages (0.2.3)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from torchist) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->torchist) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->torchist) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->torchist) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->torchist) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->torchist) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->torchist) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.6.0->torchist) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->torchist) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "afVxel6Zp1Iy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fvcore.nn import FlopCountAnalysis\n",
        "import torch\n",
        "import torchist\n",
        "import torch.nn as nn\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import mnist\n",
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "22rRtL5ySC4t"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load an image from MNIST to use in the report B"
      ],
      "metadata": {
        "id": "bV3Jgxwxl4KA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Selected an image to display\n",
        "image_index = 0\n",
        "image = train_images[image_index]\n",
        "label = train_labels[image_index]\n",
        "\n",
        "# Display the image\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.title(f'Label: {label}')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "mftxaHM8d0kO",
        "outputId": "74bba056-e675-47f5-9036-6d66a7a1eaf9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a baseline ViT model with 4 Transformer Encoder Layers to compare with the DD ViT model"
      ],
      "metadata": {
        "id": "VDv5PMiYmENr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "c36nUDtQR6G0"
      },
      "outputs": [],
      "source": [
        "class BaselineTransformer(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Linear(28, input_dim)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=4, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=4) # Assign 4 Transformer Encoder Layers for the baseline model\n",
        "        self.fc = nn.Linear(input_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.squeeze(1)  # Remove the channel dimension: (batch_size, 28, 28)\n",
        "        x = x.permute(0, 2, 1)  # Reshape to (batch_size, length_seq, input_dim)\n",
        "        x = self.embedding(x)\n",
        "        x = self.transformer(x)\n",
        "        x = x.mean(dim=1)  # Global average pooling\n",
        "        return self.fc(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the DynamicDepthVisualEncoding class with entropy complexity calculation to determine the number of Transformer Encoder Layers"
      ],
      "metadata": {
        "id": "aPXOiTPMj07x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DyanimcDepthVisualEncoding(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes, max_layers=4):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Linear(28, input_dim)\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            nn.TransformerEncoderLayer(d_model=input_dim, nhead=4, batch_first=True)\n",
        "            for _ in range(max_layers)\n",
        "        ])\n",
        "        self.fc = nn.Linear(input_dim, num_classes)\n",
        "\n",
        "    def compute_entropy(self, x):\n",
        "        # Flatten the image to compute histogram\n",
        "        x_flat = x.view(x.size(0), -1)\n",
        "        entropy_values = []\n",
        "        for img in x_flat:\n",
        "            # Normalize the embedded inputs between 0 and 1\n",
        "            img_min, img_max = img.min(), img.max()\n",
        "            img_normalized = (img - img_min) / (img_max - img_min)\n",
        "\n",
        "            # Compute histogram in [0, 1] range\n",
        "            hist = torchist.histogramdd(img_normalized.unsqueeze(1), bins=256, low=0.0, upp=1.0)\n",
        "            # Normalize histogram to get a probability distribution\n",
        "            prob_dist = hist / hist.sum()\n",
        "            # Compute entropy\n",
        "            entropy = -torch.sum(prob_dist * torch.log2(prob_dist + 1e-9))\n",
        "            entropy_values.append(entropy)\n",
        "        return torch.stack(entropy_values)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.squeeze(1)  # Remove the channel dimension: (batch_size, 28, 28)\n",
        "        x = x.permute(0, 2, 1)  # Reshape to (batch_size, length_seq, input_dim)\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # Compute entropy-based complexity\n",
        "        entropy = self.compute_entropy(x)\n",
        "\n",
        "        # Normalize entropy to range [0, 1]\n",
        "        entropy_normalized = entropy/8.0\n",
        "        # Determine number of layers to use based on normalized entropy\n",
        "        num_layers_to_use = torch.clamp((entropy_normalized * len(self.encoder_layers)).long(), 1, len(self.encoder_layers))\n",
        "        for i, layer in enumerate(self.encoder_layers):\n",
        "            if (i + 1) > num_layers_to_use.max():\n",
        "                break\n",
        "            x = layer(x)\n",
        "\n",
        "        x = x.mean(dim=1) # Global average pooling\n",
        "        return self.fc(x)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b-7zLR7omSmJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the mnist dataset and split into training and testing for both baseline and DD ViT model"
      ],
      "metadata": {
        "id": "knsXLn1mkR-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mnist_dataloaders(batch_size=64):\n",
        "    # Transform the images to normalize them\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))  # Normalize to mean 0 and std 1\n",
        "    ])\n",
        "\n",
        "    # Download dataset\n",
        "    train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "    test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "    # Define proper dataloaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, test_loader\n"
      ],
      "metadata": {
        "id": "uymAQ8nkR9w6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and evaluation loop for both models"
      ],
      "metadata": {
        "id": "nPALB0UZkd_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, criterion, optimizer, epochs=5):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.train() # Train model\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        current_loss = 0.0\n",
        "        right = 0\n",
        "        total = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)  # Move inputs and labels to the device\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            current_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            right += predicted.eq(labels).sum().item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {current_loss/len(train_loader):.4f}, Accuracy: {100*right/total:.2f}%\")\n",
        "\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval() # Evaluate model\n",
        "\n",
        "    right = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)  # Move inputs and labels to the device\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            right += predicted.eq(labels).sum().item()\n",
        "\n",
        "    print(f\"Test Accuracy: {100*right/total:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "mtvlG-59R_Cy"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple method to calculate the number of floating-point operations per second (FLOPs)"
      ],
      "metadata": {
        "id": "of0ScX0gmg8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_flops(model, dummy_input):\n",
        "    flops = FlopCountAnalysis(model, dummy_input)\n",
        "    print(f\"FLOPs: {flops.total()}\")\n",
        "    return flops.total()\n"
      ],
      "metadata": {
        "id": "o2MmqhqPSAae"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Define the parameters\n",
        "    batch_size = 64\n",
        "    input_dim = 64\n",
        "    num_classes = 10\n",
        "    epochs = 5\n",
        "    learning_rate = 0.001\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Load data from MNIST dataset\n",
        "    train_loader, test_loader = get_mnist_dataloaders(batch_size)\n",
        "\n",
        "    # Define dummy input for FLOP calculation and put it to device\n",
        "    dummy_input = torch.rand(batch_size, 1, 28, 28).to(device)\n",
        "\n",
        "    # Train and evaluate Baseline Model using crossentropy loss and the Adam optimizer for simplicity\n",
        "    print(\"Training Baseline Model:\")\n",
        "    baseline_model = BaselineTransformer(input_dim, num_classes)\n",
        "    optimizer = optim.Adam(baseline_model.parameters(), lr=learning_rate)\n",
        "    train_model(baseline_model, train_loader, criterion, optimizer, epochs)\n",
        "\n",
        "    print(\"Evaluating Baseline Model:\")\n",
        "    evaluate_model(baseline_model, test_loader)\n",
        "\n",
        "    print(\"FLOPs for Baseline Model: \")\n",
        "    calculate_flops(baseline_model, dummy_input)\n",
        "\n",
        "    # Train and evaluate DD ViT using same crossentropy loss formula and the Adam optimizer\n",
        "    # Parameters remain the same for proper comparison\n",
        "    print(\"\\nTraining DD ViT:\")\n",
        "    dd_vit_model = DyanimcDepthVisualEncoding(input_dim, num_classes)\n",
        "    optimizer = optim.Adam(dd_vit_model.parameters(), lr=learning_rate)\n",
        "    train_model(dd_vit_model, train_loader, criterion, optimizer, epochs)\n",
        "\n",
        "    print(\"Evaluating DD ViT Model...\")\n",
        "    evaluate_model(dd_vit_model, test_loader)\n",
        "\n",
        "    print(\"FLOPs for DD ViT Model...\")\n",
        "    calculate_flops(dd_vit_model, dummy_input)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFFx7X7YSB2O",
        "outputId": "916e3631-5ae6-4d96-e87e-18f99666ae62"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Baseline Model:\n",
            "Epoch 1/5, Loss: 0.5315, Accuracy: 82.61%\n",
            "Epoch 2/5, Loss: 0.2582, Accuracy: 92.08%\n",
            "Epoch 3/5, Loss: 0.2033, Accuracy: 93.54%\n",
            "Epoch 4/5, Loss: 0.1744, Accuracy: 94.62%\n",
            "Epoch 5/5, Loss: 0.1578, Accuracy: 95.08%\n",
            "Evaluating Baseline Model:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::div encountered 4 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::unflatten encountered 4 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mul encountered 16 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::scaled_dot_product_attention encountered 4 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add encountered 8 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mean encountered 1 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
            "transformer.layers.0.self_attn.out_proj, transformer.layers.1.self_attn.out_proj, transformer.layers.2.self_attn.out_proj, transformer.layers.3.self_attn.out_proj\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 94.73%\n",
            "FLOPs for Baseline Model: \n",
            "FLOPs: 2004328448\n",
            "\n",
            "Training DD ViT:\n",
            "Epoch 1/5, Loss: 0.4631, Accuracy: 85.22%\n",
            "Epoch 2/5, Loss: 0.2252, Accuracy: 92.94%\n",
            "Epoch 3/5, Loss: 0.1800, Accuracy: 94.36%\n",
            "Epoch 4/5, Loss: 0.1550, Accuracy: 95.10%\n",
            "Epoch 5/5, Loss: 0.1415, Accuracy: 95.50%\n",
            "Evaluating DD ViT Model...\n",
            "Test Accuracy: 95.50%\n",
            "FLOPs for DD ViT Model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::div encountered 3 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::unflatten encountered 3 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mul encountered 12 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::scaled_dot_product_attention encountered 3 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add encountered 6 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mean encountered 1 time(s)\n",
            "WARNING:fvcore.nn.jit_analysis:The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
            "encoder_layers.0.self_attn.out_proj, encoder_layers.1.self_attn.out_proj, encoder_layers.2.self_attn.out_proj, encoder_layers.3, encoder_layers.3.dropout, encoder_layers.3.dropout1, encoder_layers.3.dropout2, encoder_layers.3.linear1, encoder_layers.3.linear2, encoder_layers.3.norm1, encoder_layers.3.norm2, encoder_layers.3.self_attn, encoder_layers.3.self_attn.out_proj\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FLOPs: 1504059392\n"
          ]
        }
      ]
    }
  ]
}